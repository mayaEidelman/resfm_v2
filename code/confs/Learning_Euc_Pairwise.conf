exp_name = Learning_Euc_Pairwise
random_seed=20
dataset
{
     dataset = "megadepth"
    use_gt = False
    remove_outliers_gt = False
    calibrated = True
    batch_size = 4
    min_sample_size = 0.1
    max_sample_size = 0.2


    # For training
    train_set = ["0156 new", "0860 new", "0412 new", "0217 new", "0181 new", "0285 new", "0214 new", "0041 new", "0275 new", "0186 new", "0474 new", "0476 new", "0768 new", "0204 new", "0733 new", "0047 new", "0229 new", "0175 new", "0349 new", "0107 new", "0303 new", "0058 new", "0286 new", "0062 new", "0478 new", "0271 new", "0360 new"]
    validation_set = ["5015 new", "0176_300 new", "0299_300 new", "0290 new"]
    test_set = ["5015 new"]

    # For debugging
    train_set = ["0012_300 new"]
    validation_set = ["0012_300 new"]
    test_set = ["0012_300 new"]
}
model
{
    type = SetOfSet.SetOfSetNet
    num_features = 256
    num_blocks = 1
    block_size = 3
    use_skip = False
    multires = 0
}
train
{
    lr = 0.001
    num_of_epochs = 100
    scheduler_milestone = [20000]
    gamma = 0.5
    eval_intervals = 250
    optimization_num_of_epochs = 500
    optimization_eval_intervals = 250
    optimization_lr = 1e-3
    validation_metric = ["Accuracy"]
    validation_metric_fine_tuning = ["our_repro"]
}
ba
{
run_ba = True
repeat=True
triangulation=False  # If repeat, the first time is from our points and the second from triangulation
only_last_eval = True
}
loss
{
    func = CombinedLoss
    infinity_pts_margin = 1e-3
    reproj_loss_weight= 1.0
    classification_loss_weight = 0.1
    normalize_grad = True
    hinge_loss = True
    hinge_loss_weight = 1
    
    # Unsupervised pairwise consistency loss weights
    esfm_weight = 1.0
    pairwise_weight = 0.5
    epipolar_weight = 1.0
    geometric_weight = 0.5
} 
test
{
    outliers_threshold = 0.6
}